import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from statsmodels.graphics.mosaicplot import mosaic
from itertools import product
from sklearn.compose import make_column_transformer
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import OneHotEncoder
from aif360.datasets import CompasDataset
from aif360.datasets import BinaryLabelDataset
from aif360.sklearn.datasets import fetch_compas
from aif360.algorithms.inprocessing import MetaFairClassifier, PrejudiceRemover, ARTClassifier
from aif360.sklearn.preprocessing import ReweighingMeta
from aif360.sklearn.postprocessing import CalibratedEqualizedOdds, PostProcessingMeta
from aif360.sklearn.metrics import disparate_impact_ratio, average_odds_difference, statistical_parity_difference, equal_opportunity_difference
# from aif360.algorithms.preprocessing import DisparateImpactRemover
# from BlackBoxAuditing.repairers.GeneralRepairer import Repairer
from IPython.display import Markdown, display
## load in data
## In machine learning tasks, specifically with supervised learning, you have features and labels.
## The features are the descriptive attributes (they are defined as X), and the label (y) is what you're attempting to predict or forecast
X, y = fetch_compas()
print(f'There are {X.shape[0]} entries and {X.shape[1]} features')
X.head()
## because our analysis is mainly focusing on how the algorithm treats white and Black people differently, we are
## dropping the rows of data where race != Caucasian or African American
X_new = X[(X.race == "Caucasian") | (X.race == "African-American")]
print(f'There are {X_new.shape[0]} entries and {X_new.shape[1]} features')
X_new.head()
## drop unused race categories
# list of categories to be removed
X_new["race"] = X_new["race"].cat.remove_unused_categories()
y_new = y[(y.index.get_level_values(2) == "Caucasian") | (y.index.get_level_values(2) == "African-American")]
y_new.head()
# Function for visualising the confusion matrix and other statistics
# https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py
def make_confusion_matrix(cf_matrix, model):
group_names = ["True Negative","False Positive","False Negative","True Positive"]
group_counts = ["{0:0.0f}".format(value) for value in
cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
cf_matrix.flatten()/np.sum(cf_matrix)]
group_labels = ["{}\n".format(value) for value in group_names]
group_counts = ["{0:0.0f}\n".format(value) for value in cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]
box_labels = [f"{v1}{v2}{v3}".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]
box_labels = np.asarray(box_labels).reshape(cf_matrix.shape[0],cf_matrix.shape[1])
# add more statistics
accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))
precision = cf_matrix[1,1] / sum(cf_matrix[:,1])
recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])
f1_score  = 2*precision*recall / (precision + recall)
stats_text = "\n\nAccuracy={:0.3f}\nPrecision={:0.3f}\nRecall={:0.3f}\nF1 Score={:0.3f}".format(
accuracy,precision,recall,f1_score)
categories=["Survived", "Recidivated"]
sns.heatmap(cf_matrix,annot=box_labels,fmt="",cmap='Purples',xticklabels=categories,yticklabels=categories)
plt.ylabel('True label')
plt.xlabel('Predicted label' + stats_text)
plt.title(f"Confusion matrix and statistics for the {model} model");
## defining function for displaying metrics of training and test data by race
def metrics_per_group(y_test, y_pred):
# y true per group
y_test_white = y_test.loc[y_test.index.get_level_values(2) == 1]
y_test_black = y_test.loc[y_test.index.get_level_values(2) == 0]
# y_pred per group
y_pred_white = y_pred[y_test.index.get_level_values(2) == 1]
y_pred_black = y_pred[y_test.index.get_level_values(2) == 0]
# metrics
scores = []
scores.append(accuracy_score(y_test, y_pred))
scores.append(recall_score(y_test, y_pred))
scores.append(precision_score(y_test, y_pred))
scores.append(accuracy_score(y_test_black, y_pred_black))
scores.append(recall_score(y_test_black, y_pred_black))
scores.append(precision_score(y_test_black, y_pred_black))
scores.append(accuracy_score(y_test_white, y_pred_white))
scores.append(recall_score(y_test_white, y_pred_white))
scores.append(precision_score(y_test_white, y_pred_white))
attribute = ["all"]*3 + ["black"] *3 + ["white"] *3
metric = ["accuracy", "recall", "precision"] * 3
# dictionary of lists
dict = {'race': attribute, 'metrics': metric, 'score': scores}
df = pd.DataFrame(dict)
sns.barplot(x = "metrics", y = "score", hue = "race", data = df, palette = ['#dfcd1a', '#9d0677', '#236c48'])
plt.title("Performance metrics by groups")
def plot_fair_metrics(fair_metrics_mitigated, model):
cols = ['statistical_parity_difference','equal_opportunity_difference','average_odds_difference','disparate_impact_ratio']
obj_fairness = [[0,0,0,1]]
# row for objectives
fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)
# row for baseline model
fair_metrics.loc['Baseline Model'] = [stat_par_diff, eq_opp_diff, avg_odds_diff, disp_impact_ratio]
# row for mitigated bias
fair_metrics.loc[model] = fair_metrics_mitigated
metrics_len = len(cols)
fig, ax = plt.subplots(figsize=(20,4), ncols=metrics_len, nrows=1)
plt.subplots_adjust(
left    =  0.125,
bottom  =  0.1,
right   =  0.9,
top     =  0.9,
wspace  =  .5,
hspace  =  1.1
)
y_title_margin = 1.2
plt.suptitle("Fairness metrics", y = 1.09, fontsize=20)
sns.set(style="dark")
cols = fair_metrics.columns.values
obj = fair_metrics.loc['objective']
size_rect = [0.2,0.2,0.2,0.4]
rect = [-0.1,-0.1,-0.1,0.8]
bottom = [-1,-1,-1,0]
top = [1,1,1,2]
bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.25]]
for i in range(0,metrics_len):
plt.subplot(1, metrics_len, i+1)
ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])
for j in range(0,len(fair_metrics)-1):
a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]
marg = -0.2 if val < 0 else 0.1
ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')
plt.ylim(bottom[i], top[i])
plt.setp(ax.patches, linewidth=0)
ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor="green", linewidth=1, linestyle='solid'))
plt.axhline(obj[i], color='black', alpha=0.3)
plt.title(cols[i])
ax.set_ylabel('')
ax.set_xlabel('')
X_new.index = pd.MultiIndex.from_arrays(X_new.index.codes, names=X_new.index.names)
y_new.index = pd.MultiIndex.from_arrays(y_new.index.codes, names=y_new.index.names)
# 0 is African American, 2 is Caucasian
# set caucasian equal to 1 instead of 2
X_new = X_new.rename(index={2: 1}, level='race')
X_new
# set target class to 0/1
y_new = pd.Series(y_new.factorize(sort=True)[0], index=y_new.index)
# set caucasian equal to 1 instead of 2
y_new = y_new.rename(index={2: 1}, level='race')
y_new
# needs interpretation
X_new_index = X_new.rename(columns={"race": "def_race"})
X_new_index = X_new_index.rename(columns={"sex": "def_sex"})
# renaming dataframe columns to avoid the ValueError of variables being a column AND index label (which is ambiguous)
X_new_index.groupby(["def_race"])["age"].median()
X_new_index.groupby(["def_race", "sex"]).size()
X_new_index.groupby(["def_race", "c_charge_degree"]).size()
X_new_index.groupby(["def_race"])["priors_count"].median()
#X_new_index.groupby(["def_race"])["juv_fel_count"].median()
#X_new_index.groupby(["def_race"])["juv_misd_count"].median()
# above two lines commented out since medians for everyone = 0
df_viz = X_new.copy()
df_viz['race'] = X_new['race'].replace({1.0: 'Caucasian', 0.0: 'African-American'})
df_viz['two_year_recid'] = y_new.replace({1:'Recidivated', 0: 'Survived'})
df_viz.index = df_viz.index.droplevel('race')
purple = '#9d0677'
green = '#30875c'
orange = '#E7881E'
blue = '#20A4CF'
workshop_palette = [purple, green]
df_viz.head()
crosstable=pd.crosstab(df_viz['race'],df_viz['c_charge_degree'])
crosstable
props={}
props[('Caucasian','F')]={'facecolor':'red', 'edgecolor':'white'}
props[('Caucasian','M')]={'facecolor':'red', 'edgecolor':'white'}
props[('African-American','F')]={'facecolor':'xkcd:aqua','edgecolor':'white'}
props[('African-American','M')]=        {'facecolor':'xkcd:aqua','edgecolor':'white'}
labelizer=lambda k:{('Caucasian','F'):1242,('African-American','F'):2194,('Caucasian','M'):858,('African-American','M'):979}[k]
mosaic(df_viz,['race','c_charge_degree'],labelizer=labelizer,properties=props)
plt.show()
# barplot of recividism
sns.countplot(x='two_year_recid', data=df_viz, palette=workshop_palette)
plt.title('Two Year Recidivism Rate')
crosstable=pd.crosstab(df_viz['race'],df_viz['two_year_recid'])
crosstable
props={}
props[('Caucasian','Recidivated')]={'facecolor':'red', 'edgecolor':'white'}
props[('Caucasian','Survived')]={'facecolor':'red', 'edgecolor':'white'}
props[('African-American','Recidivated')]={'facecolor':'xkcd:aqua','edgecolor':'white'}
props[('African-American','Survived')]=        {'facecolor':'xkcd:aqua','edgecolor':'white'}
labelizer=lambda k:{('Caucasian','Recidivated'):1242,('African-American','Recidivated'):1661,('Caucasian','Survived'):858,('African-American','Survived'):1512}[k]
mosaic(df_viz,['race','two_year_recid'],labelizer=labelizer,properties=props)
plt.show()
props={}
props[('Caucasian','Recidivated')]={'facecolor':'red', 'edgecolor':'white'}
props[('Caucasian','Survived')]={'facecolor':'red', 'edgecolor':'white'}
props[('African-American','Recidivated')]={'facecolor':'xkcd:aqua','edgecolor':'white'}
props[('African-American','Survived')]=        {'facecolor':'xkcd:aqua','edgecolor':'white'}
labelizer=lambda k:{('Caucasian','Recidivated'):822,('African-American','Recidivated'):1661,('Caucasian','Survived'):1278,('African-American','Survived'):1512}[k]
mosaic(df_viz,['race','two_year_recid'],labelizer=labelizer,properties=props)
plt.show()
library(reticulate)
# for Dianne
use_condaenv("base")
# for Amrita
#use_condaenv("anaconda3")
conda install -c conda-forge wordcloud
library(reticulate)
# for Dianne
use_condaenv("base")
# for Amrita
#use_condaenv("anaconda3")
library(reticulate)
# for Dianne
use_condaenv("base")
library(reticulate)
# for Dianne
use_condaenv("base")
library(reticulate)
# for Dianne
use_condaenv("base")
library(reticulate)
# for Dianne
use_condaenv("base")
# for Amrita
#use_condaenv("anaconda3")
